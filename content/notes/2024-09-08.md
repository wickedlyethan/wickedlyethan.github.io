---
title: '2024-09-08'
---
David Pierce of *The Verge* shared a piece by AI researcher Arlie Coles exploring [the intersection of large language models and Christianity](https://www.plough.com/en/topics/life/technology/chatgpt-goes-to-church), and while the piece gets very liturgical I think there's a few pieces of great writing and thinking that touch on some of what I dislike so greatly about so-called "AI".

Firstly, the best summation of what language models are, emphasis mine:

> Researchers know – but seldom effectively communicate – that indispensable LM applications sit just one level deeper than the “type query, get content” chatbot paradigm. That’s because **LMs are only accidentally content machines; they are *substantially* a dense statistical representation of relationships between words**.

That's it! That's all they are! Any "intelligence" they portray we have personified from their output, which becomes easy to do because of the power of language, not the power of computers.

Meanwhile Coles lists genuinely positive uses of machine learning and language models – "helping to [decipher dead languages](https://aclanthology.org/P19-1303/), [restore lost ancient inscriptions](https://dl.acm.org/doi/full/10.1145/3593431), and [predict protein structures](https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/)" – and lays the blame for their worse uses at our own feet:

> But none of these mentioned applications use LMs as cheap content machines; they are harder to understand (and get less press) than the instant feedback an impressive or appalling chatbot provides. Our attention spans are short; our demand for content is high. The high-strung discourse around LMs is, in a sense, what we deserve.

(This echoes philosopher L.M. Sacasas' own, if even more overtly moralistic, view of us: "For my part, if I have a disordered relationship with the internet, I know, in the immortal words of Jimmy Buffett, that it’s my own damn fault." I [wrote]({{<ref "/blog/2024-03-28 Dopamine Was Never Enough.md">}}) about how I disagree with how that sentiment frames us as the problem instead of the system we inhabit, but I understand it.)

Lastly, with some light cheeky editing:

> The fixation on LMs as content generators, tools that circumvent the necessity of thinking together, is symptomatic of a deeper disease, developing out of our failure to integrate our unprecedented technological interconnectedness with the bodily realities that ~~true Christian~~ – true human – interdependence demands.

7 years ago (oh no) host Mike Rugnetta closed out PBS Idea Channel with an essay entreating us all to [think out loud with each other](https://youtu.be/sQ0pny1TA6U?si=29_oNtfbNn19A4Wi) and quotes scholar Donna Haraway: "It matters which stories tell stories... it matters what ideas we use to think other ideas with." He tells us to "think about one another and believe each other". How can we do that if language models, so-called "AI", are doing shitty thinking for us? What is the story of "AI", and how does that affect what ideas we use it to think for us? When we use them, do we think about anybody at all?

My wife recently had to sit through professional development recently where the school principal presented bad slides. In them she uncritically offered a world-salad definition for "student belonging" generated, and credited to, Google's AI generative overview. The LM did the thinking for her and robbed the entire audience of getting to think with each other, and that's grim.