---
title: "A (Really Dumb) Ghost in the Machine"
date: 2024-09-08
updated:
summary: So-called "AI" stop us from thinking together or about others. What happens when they show up everywhere? A piece about the intersection of large language models and Christianity of all things has some good points.
---
David Pierce of *The Verge* recently shared AI researcher Arlie Coles exploring [the intersection of large language models and Christianity](https://www.plough.com/en/topics/life/technology/chatgpt-goes-to-church), and while the piece gets very liturgical, I think there's a few pieces of great writing and thinking that touch on some of what I dislike so greatly about so-called "AI".

Firstly, the best summation of what language models are, emphasis mine:

> Researchers know – but seldom effectively communicate – that indispensable LM applications sit just one level deeper than the “type query, get content” chatbot paradigm. That’s because **LMs are only accidentally content machines; they are *substantially* a dense statistical representation of relationships between words**.

That's it! That's all they are! Any "intelligence" they portray we have personified from their output, which becomes easy to do because of the power of language, not the power of computers.

Meanwhile Coles lists genuinely positive uses of machine learning and language models – "helping to [decipher dead languages](https://aclanthology.org/P19-1303/), [restore lost ancient inscriptions](https://dl.acm.org/doi/full/10.1145/3593431), and [predict protein structures](https://www.scientificamerican.com/article/one-of-the-biggest-problems-in-biology-has-finally-been-solved/)" – and lays the blame for their worse uses at our own feet:

> But none of these mentioned applications use LMs as cheap content machines; they are harder to understand (and get less press) than the instant feedback an impressive or appalling chatbot provides. Our attention spans are short; our demand for content is high. The high-strung discourse around LMs is, in a sense, what we deserve.

This echoes philosopher L.M. Sacasas' own, if even more overtly moralistic, view of us: "For my part, if I have a disordered relationship with the internet, I know, in the immortal words of Jimmy Buffett, that it’s my own damn fault." I [wrote]({{<ref "/blog/2024-03-28 Dopamine Was Never Enough.md">}}) about how I disagree with how that sentiment frames us as the problem instead of the system we inhabit, but I understand it.

Case in point, I'm updating this post and converting it from a note into a blog post on the day of Apple's iPhone 16 announcement, and a central selling point of the fleet of new phones is Apple's grafting of generative AI tools [into the operating system](https://www.theverge.com/2024/9/10/24237714/apple-intelligence-generative-ai-features-update-schedule). As Pierce himself notes on the related episode of the *Vergecast*, someday our emails are going to be re-written by AI and sent to another AI which will summarize it back down to notify the person you sent it to, who will craft a response then have an AI re-write it anyways, and the cycle starts again. People write shit emails because there's no reason for them to write better ones, not because they don't have the tools to do so; no one asked for a system to write better ones, they're getting shoved into the OSes we inhabit. What happens if the expectation becomes that emails become weirdly verbose, stiffly-generated fluff instead?

Back to Coles' piece. Lastly, with some atheistic editing:

> The fixation on LMs as content generators, tools that circumvent the necessity of thinking together, is symptomatic of a deeper disease, developing out of our failure to integrate our unprecedented technological interconnectedness with the bodily realities that ~~true Christian~~ – true human – interdependence demands.

7 years ago (oh no) host Mike Rugnetta closed out PBS Idea Channel with an essay entreating us all to [think out loud with each other](https://youtu.be/sQ0pny1TA6U?si=29_oNtfbNn19A4Wi) and quotes scholar Donna Haraway: "It matters which stories tell stories... it matters what ideas we use to think other ideas with." He tells us to "think about one another and believe each other". How can we do that if language models, so-called "AI", are doing shitty thinking for us? What is the story of "AI", and how does that affect what ideas we use it to think for us? When we use them, do we think about anybody at all?

My wife recently had to sit through professional development recently where the school principal presented bad slides. In them she uncritically offered a world-salad definition for "student belonging" generated, and credited to, Google's AI generative overview. The LM did the thinking for her and robbed the entire audience of getting to think with each other, and that's grim.

(I have more thoughts on this, I'll update this piece as more come.)
